{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509ac993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad795775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(enum.IntEnum):\n",
    "    Buy_100 = 0\n",
    "    Buy_10 = 1\n",
    "    Skip = 2\n",
    "    Sell_10 = 3\n",
    "    Sell_100 = 4\n",
    "    #環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc964c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_BARS_COUNT = 10\n",
    "DEFAULT_COMMISSION = 0.001\n",
    "\n",
    "class StocksEnv:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.bars_count = DEFAULT_BARS_COUNT\n",
    "        self.commission = DEFAULT_COMMISSION\n",
    "        self.shape = self.bars_count*24+1 \n",
    "    \n",
    "    def reset(self):\n",
    "        self.fund = 100000\n",
    "        self.hold = 0\n",
    "        self.buy_price = []\n",
    "        self.instrument = np.random.choice(list(self.data.keys()))\n",
    "        self.prices = self.data[self.instrument]\n",
    "        self.offset = np.random.choice(self.prices.open.shape[0]-self.bars_count*10) + self.bars_count - 1\n",
    "        obs = self.encode()\n",
    "        return obs\n",
    "    \n",
    "    def reset_val(self, index):\n",
    "        self.fund = 100000\n",
    "        self.hold = 0\n",
    "        self.buy_price = []\n",
    "        self.instrument = list(self.data.keys())[index]\n",
    "        self.prices = self.data[self.instrument]\n",
    "        self.offset = 9\n",
    "        obs = self.encode()\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        close = self.prices.close[self.offset]\n",
    "        price = self.prices.price[self.offset]\n",
    "        self.affordable = self.fund // price\n",
    "        if action == Actions.Buy_100 and self.affordable >= 100:\n",
    "            reward -= self.commission * 10\n",
    "            self.hold += 100\n",
    "            self.fund -= price * 100\n",
    "            if int(price * 100 * 0.001425) == 0:\n",
    "                tax = 1\n",
    "            else:\n",
    "                tax = int(price * 100 * 0.001425)\n",
    "            self.fund -= tax\n",
    "            self.buy_price.extend([price]*10)\n",
    "        elif action == Actions.Buy_10 and self.affordable >= 10:\n",
    "            reward -= self.commission\n",
    "            self.hold += 10\n",
    "            self.fund -= price * 10\n",
    "            if int(price * 10 * 0.001425) == 0:\n",
    "                tax = 1\n",
    "            else:\n",
    "                tax = int(price * 10 * 0.001425)\n",
    "            self.fund -= tax\n",
    "            self.buy_price.extend([price])\n",
    "        elif action == Actions.Sell_100 and self.hold >= 100:\n",
    "            reward -= self.commission * 10\n",
    "            self.hold -= 100\n",
    "            self.fund += price * 100\n",
    "            if int(price * 100 * 0.001425) == 0:\n",
    "                tax = 1\n",
    "            else:\n",
    "                tax = int(price * 100 * 0.001425)\n",
    "            self.fund -= (tax + int(price*100*0.003))\n",
    "            for i in range(10):\n",
    "                reward += 100.0 * (price - self.buy_price[0]) / self.buy_price[0]\n",
    "                self.buy_price.pop(0)\n",
    "        elif action == Actions.Sell_10 and self.hold >= 10:\n",
    "            reward -= self.commission\n",
    "            self.hold -= 10\n",
    "            self.fund += price * 10\n",
    "            if int(price * 10 * 0.001425) == 0:\n",
    "                tax = 1\n",
    "            else:\n",
    "                tax = int(price * 10 * 0.001425)\n",
    "            self.fund -= (tax + int(price*10*0.003))\n",
    "            reward += 100.0 * (price - self.buy_price[0]) / self.buy_price[0]\n",
    "            self.buy_price.pop(0)\n",
    "        self.offset += 1\n",
    "        prev_price = price\n",
    "        price = self.prices.price[self.offset]\n",
    "        if self.offset >= self.prices.close.shape[0]-1:\n",
    "            done = True\n",
    "        if self.hold > 0:\n",
    "            reward += 100.0 * (price - prev_price) / prev_price * self.hold / 10\n",
    "             \n",
    "        obs = self.encode()\n",
    "        info = {\"instrument\": self.instrument, \"offset\": self.offset}\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def encode(self):\n",
    "        obs = np.ndarray(shape=(self.shape), dtype=np.float32)\n",
    "        shift = 0\n",
    "        for bar_idx in range(-self.bars_count+1, 1):\n",
    "            obs[shift] = self.prices.open[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.high[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.low[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.close[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fa[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fafive[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.faten[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fatwenty[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fvolume[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fvolumefive[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fvolumeten[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fvolumetwenty[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fbb[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fbbten[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fbbtwenty[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fv[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fvfive[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fvten[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.frsv[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fk[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fd[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.frsi[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.fubb[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "            obs[shift] = self.prices.flbb[self.offset + bar_idx]\n",
    "            shift += 1\n",
    "        obs[shift] = self.hold\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8cd214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_n, out_n):\n",
    "        super(DDQN, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_n, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f1c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# _*_ coding:utf-8 _*_\n",
    "#更改文件编码，文件统一改为utf-8无BOM格式\n",
    "import os\n",
    "from chardet import detect\n",
    "\n",
    "#文件夹目录\n",
    "g_filedir = r'C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2'\n",
    "\n",
    "def runcoding(path):\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(path,filename),'rb+') as fileObj:\n",
    "                fileContent = fileObj.read()\n",
    "                #判断编码格式\n",
    "                encodingtype = detect(fileContent)['encoding']\n",
    "               \n",
    "                print(encodingtype)\n",
    "                #格式转换\n",
    "                fileContent = fileContent.decode(encodingtype).encode('utf8')\n",
    "                #写回文件\n",
    "                fileObj.seek(0)\n",
    "                fileObj.write(fileContent)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    runcoding(g_filedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08457164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2\\2303train.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2\\2308train.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2\\2317train.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2\\2330train.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2\\2357train.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2\\2379train.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2\\2382train.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2\\2395train.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2303val.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2308val.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2317val.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2327val.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2330val.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2357val.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2379val.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2382val.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2395val.csv\n",
      "Read done\n",
      "Reading C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\\2408val.csv\n",
      "Read done\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_train_data = \"C:/Users/yuanf/OneDrive/桌面/半導體股票資料/train2\"\n",
    "DEFAULT_val_data = \"C:/Users/yuanf/OneDrive/桌面/半導體股票資料/val2\"\n",
    "Prices = namedtuple('prices', ('open', 'high', 'low', 'close', 'fa', 'fafive', 'faten', 'fatwenty', 'fvolume', 'fvolumefive', 'fvolumeten', 'fvolumetwenty', 'fbb', 'fbbten', 'fbbtwenty', 'fv', 'fvfive', 'fvten', 'frsv', 'fk', 'fd', 'frsi', 'fubb', 'flbb', 'price'))\n",
    "\n",
    "def data_files(dir_name):\n",
    "    result = []\n",
    "    for path in glob.glob(os.path.join(dir_name, \"*.csv\")):\n",
    "        result.append(path)\n",
    "    return result\n",
    "\n",
    "def read_csv(file_name, sep = ','):\n",
    "    print(\"Reading\", file_name)\n",
    "    with open(file_name, 'rt', encoding='utf-8') as fd:\n",
    "        reader = csv.reader(fd, delimiter=sep)\n",
    "        h = next(reader)\n",
    "        if 'Open' not in h and sep == ',':\n",
    "            return read_csv(file_name, ';')\n",
    "        indices = [h.index(s) for s in ('Open', 'High', 'Low', 'Close', 'A', 'A5', 'A10', 'A20', 'Capacity', 'Capacity5', 'Capacity10', 'Capacity20', 'BB', 'BB10', 'BB20', 'V', 'V5', 'V10', 'rsv', 'K', 'D', 'rsi', 'UBB', 'LBB')]\n",
    "        o, h, l, c, a, afive, aten, atwenty, volume, volumefive, volumeten, volumetwenty, bb, bbten, bbtwenty, v, vfive, vten, rsv, k, d, rsi, ubb, lbb = [], [], [], [], [],[], [], [], [], [],[], [], [], [], [],[], [], [], [], [],[], [], [], []\n",
    "        for row in reader:\n",
    "            vals = list(map(float, [row[idx] for idx in indices]))\n",
    "            po, ph, pl, pc,pa, pafive, paten, patwenty, pvolume, pvolumefive, pvolumeten, pvolumetwenty, pbb, pbbten, pbbtwenty, pv, pvfive, pvten, prsv, pk, pd, prsi, pubb, plbb  = vals\n",
    "            o.append(po)\n",
    "            c.append(pc)\n",
    "            h.append(ph)\n",
    "            l.append(pl)\n",
    "            a.append(pa)\n",
    "            afive.append(pafive)\n",
    "            aten.append(paten)\n",
    "            atwenty.append(patwenty)\n",
    "            volume.append(pvolume)\n",
    "            volumefive.append(pvolumefive)\n",
    "            volumeten.append(pvolumeten)\n",
    "            volumetwenty.append(pvolumetwenty)\n",
    "            bb.append(pbb)\n",
    "            bbten.append(pbbten)\n",
    "            bbtwenty.append(pbbtwenty)\n",
    "            v.append(pv)\n",
    "            vfive.append(pvfive)\n",
    "            vten.append(pvten)\n",
    "            rsv.append(prsv)\n",
    "            k.append(pk)\n",
    "            d.append(pd)\n",
    "            rsi.append(prsi)\n",
    "            ubb.append(pubb)\n",
    "            lbb.append(plbb)\n",
    "            \n",
    "    print(\"Read done\")\n",
    "    return Prices(open=np.array(o, dtype=np.float32),\n",
    "                  high=np.array(h, dtype=np.float32),\n",
    "                  low=np.array(l, dtype=np.float32),\n",
    "                  close=np.array(c, dtype=np.float32),\n",
    "                  fa=np.array(a, dtype=np.float32),\n",
    "                  fafive=np.array(afive, dtype=np.float32),\n",
    "                  faten=np.array(aten, dtype=np.float32),\n",
    "                  fatwenty=np.array(atwenty, dtype=np.float32),\n",
    "                  fvolume=np.array(volume, dtype=np.float32),\n",
    "                  fvolumefive=np.array(volumefive, dtype=np.float32),\n",
    "                  fvolumeten=np.array(volumeten, dtype=np.float32),\n",
    "                  fvolumetwenty=np.array(volumetwenty, dtype=np.float32),\n",
    "                  fbb=np.array(bb, dtype=np.float32),\n",
    "                  fbbten=np.array(bbten, dtype=np.float32),\n",
    "                  fbbtwenty=np.array(bbtwenty, dtype=np.float32),\n",
    "                  fv=np.array(v, dtype=np.float32),\n",
    "                  fvfive=np.array(vfive, dtype=np.float32),\n",
    "                  fvten=np.array(vten, dtype=np.float32),\n",
    "                  frsv=np.array(rsv, dtype=np.float32),\n",
    "                  fk=np.array(k, dtype=np.float32),\n",
    "                  fd=np.array(d, dtype=np.float32),\n",
    "                  frsi=np.array(rsi, dtype=np.float32),\n",
    "                  fubb=np.array(ubb, dtype=np.float32),\n",
    "                  flbb=np.array(lbb, dtype=np.float32),\n",
    "                  price=np.array(c, dtype=np.float32))\n",
    "\n",
    "def prices_nlz(prices):\n",
    "    o = (prices.open-prices.open.min()) / (prices.open.max()-prices.open.min())\n",
    "    h = (prices.high-prices.high.min()) / (prices.high.max()-prices.high.min())\n",
    "    l = (prices.low-prices.low.min()) / (prices.low.max()-prices.low.min())\n",
    "    c = (prices.close-prices.close.min()) / (prices.close.max()-prices.close.min())\n",
    "    a = (prices.fa-prices.fa.min()) / (prices.fa.max()-prices.fa.min())\n",
    "    afive = (prices.fafive-prices.fafive.min()) / (prices.fafive.max()-prices.fafive.min())\n",
    "    aten = (prices.faten-prices.faten.min()) / (prices.faten.max()-prices.faten.min())\n",
    "    atwenty = (prices.fatwenty-prices.fatwenty.min()) / (prices.fatwenty.max()-prices.fatwenty.min())\n",
    "    volume = (prices.fvolume-prices.fvolume.min()) / (prices.fvolume.max()-prices.fvolume.min())\n",
    "    volumefive = (prices.fvolumefive-prices.fvolumefive.min()) / (prices.fvolumefive.max()-prices.fvolumefive.min())\n",
    "    volumeten = (prices.fvolumeten-prices.fvolumeten.min()) / (prices.fvolumeten.max()-prices.fvolumeten.min())\n",
    "    volumetwenty = (prices.fvolumetwenty-prices.fvolumetwenty.min()) / (prices.fvolumetwenty.max()-prices.fvolumetwenty.min())\n",
    "    bb = (prices.fbb-prices.fbb.min()) / (prices.fbb.max()-prices.fbb.min())\n",
    "    bbten = (prices.fbbten-prices.fbbten.min()) / (prices.fbbten.max()-prices.fbbten.min())\n",
    "    bbtwenty = (prices.fbbtwenty-prices.fbbtwenty.min()) / (prices.fbbtwenty.max()-prices.fbbtwenty.min())\n",
    "    v = (prices.fv-prices.fv.min()) / (prices.fv.max()-prices.fv.min())\n",
    "    vfive = (prices.fvfive-prices.fvfive.min()) / (prices.fvfive.max()-prices.fvfive.min())\n",
    "    vten = (prices.fvten-prices.fvten.min()) / (prices.fvten.max()-prices.fvten.min())\n",
    "    rsv = (prices.frsv-prices.frsv.min()) / (prices.frsv.max()-prices.frsv.min())\n",
    "    k = (prices.fk-prices.fk.min()) / (prices.fk.max()-prices.fk.min())\n",
    "    d = (prices.fd-prices.fd.min()) / (prices.fd.max()-prices.fd.min())\n",
    "    rsi = (prices.frsi-prices.frsi.min()) / (prices.frsi.max()-prices.frsi.min())\n",
    "    ubb = (prices.fubb-prices.fubb.min()) / (prices.fubb.max()-prices.fubb.min())\n",
    "    lbb = (prices.flbb-prices.flbb.min()) / (prices.flbb.max()-prices.flbb.min())\n",
    "    p = prices.close\n",
    "    return Prices(open=o, high=h, low=l, close=c, fa=a,fafive=afive, faten=aten, fatwenty=atwenty, fvolume=volume, fvolumefive=volumefive,fvolumeten=volumeten, fvolumetwenty=volumetwenty, fbb=bb, fbbten=bbten, fbbtwenty=bbtwenty,fv=v, fvfive=vfive, fvten=vten, frsv=rsv, fk=k,fd=d, frsi=rsi, fubb=ubb, flbb=lbb, price=p)\n",
    "\n",
    "data_train = {file: prices_nlz(read_csv(file)) for file in data_files(DEFAULT_train_data)}\n",
    "data_val = {file: prices_nlz(read_csv(file)) for file in data_files(DEFAULT_val_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0fd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'state_next', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def push(self, state, action, state_next, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ae7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 100000\n",
    "GAMMA = 0.99\n",
    "\n",
    "class Brain:\n",
    "    \n",
    "    def __init__(self, input_shape, actions_n):\n",
    "        self.actions_n = actions_n\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "        \n",
    "        self.main_q_network = DDQN(input_shape, actions_n).to(device)\n",
    "        self.target_q_network = DDQN(input_shape, actions_n).to(device)\n",
    "        print(self.main_q_network)\n",
    "        self.optimizer = optim.Adam(self.main_q_network.parameters(), lr=0.0001)\n",
    "        \n",
    "    def replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.state_next_batch = self.make_minibatch()\n",
    "        self.expected_state_action_values = self.get_q_values()\n",
    "        self.update_main_q_network()\n",
    "    \n",
    "    def decide_action(self, state, episode):\n",
    "        epsilon = 1 / (episode*0.1 + 1)\n",
    "        \n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.main_q_network.eval()\n",
    "            with torch.no_grad():\n",
    "                action = self.main_q_network(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            action = torch.LongTensor([[random.randrange(self.actions_n)]]).to(device)\n",
    "        return action\n",
    "    \n",
    "    def decide_action_val(self, state):\n",
    "        self.main_q_network.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.main_q_network(state).max(1)[1].view(1, 1)\n",
    "        return action\n",
    "    \n",
    "    def make_minibatch(self):\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        state_next_batch = torch.cat(batch.state_next)\n",
    "        return batch, state_batch, action_batch, reward_batch, state_next_batch\n",
    "    \n",
    "    def get_q_values(self):\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "        \n",
    "        self.state_action_values = self.main_q_network(self.state_batch).gather(1, self.action_batch)\n",
    "        a_m = self.main_q_network(self.state_next_batch).detach().max(1)[1]\n",
    "        a_m_state_next_batch = a_m.view(-1, 1)\n",
    "        state_next_values = self.target_q_network(self.state_next_batch).gather(\n",
    "            1, a_m_state_next_batch).detach().squeeze()\n",
    "        expected_state_action_values = self.reward_batch + GAMMA * state_next_values\n",
    "        return expected_state_action_values\n",
    "    \n",
    "    def update_main_q_network(self):\n",
    "        self.main_q_network.train()\n",
    "        \n",
    "        loss = F.smooth_l1_loss(self.state_action_values, self.expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target_q_network(self):\n",
    "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a324c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, input_shape, actions_n):\n",
    "        self.brain = Brain(input_shape, actions_n)\n",
    "        \n",
    "    def update_q_function(self):\n",
    "        self.brain.replay()\n",
    "    \n",
    "    def get_action(self, state, episode):\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "    \n",
    "    def get_action_val(self, state):\n",
    "        action = self.brain.decide_action_val(state)\n",
    "        return action\n",
    "    \n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        self.brain.memory.push(state, action, state_next, reward)\n",
    "        \n",
    "    def update_target_q_function(self):\n",
    "        self.brain.update_target_q_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aefb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_EPISODES = 100000\n",
    "MAX_STEPS = 200\n",
    "VALIDATION_EVERY_STEP = 100\n",
    "\n",
    "class Enviroment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env_train = StocksEnv(data_train)\n",
    "        self.env_val = StocksEnv(data_val)\n",
    "        input_shape = DEFAULT_BARS_COUNT*24+1\n",
    "        actions_n = len(Actions)\n",
    "        self.agent = Agent(input_shape, actions_n)\n",
    "    \n",
    "    def run(self):\n",
    "        writer = SummaryWriter(comment='linear_batchsize_32_comm_0.001')\n",
    "        for episode in range(DEFAULT_EPISODES):\n",
    "            obs = self.env_train.reset()\n",
    "            train_episode_reward = 0.0\n",
    "            state = torch.tensor([obs]).to(device)\n",
    "            for step in range(MAX_STEPS):\n",
    "                action = self.agent.get_action(state, episode)\n",
    "                obs_next, reward, done, info = self.env_train.step(action.item())\n",
    "                train_episode_reward += reward\n",
    "                reward = torch.FloatTensor([reward]).to(device)\n",
    "                state_next = torch.tensor([obs_next]).to(device)\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "                self.agent.update_q_function()\n",
    "                state = state_next\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if(episode % 2 == 0):\n",
    "                self.agent.update_target_q_function()\n",
    "                \n",
    "            if(episode % VALIDATION_EVERY_STEP == 0):\n",
    "                profit = self.validation_run()\n",
    "                torch.save(self.agent.brain.main_q_network.state_dict(),\n",
    "                    r\"C:\\Users\\yuanf\\result4\\-%d\"%episode+'.pth')\n",
    "\n",
    "            writer.add_scalar('train_reward', train_episode_reward, episode)\n",
    "            writer.add_scalar('val_profit', profit, episode)\n",
    "            \n",
    "    def validation_run(self):\n",
    "        \n",
    "        funds = 0\n",
    "        for i in range(len(data_val)):\n",
    "            obs = self.env_val.reset_val(i)\n",
    "\n",
    "            while True:\n",
    "                state = torch.tensor([obs]).to(device)\n",
    "                action = self.agent.get_action_val(state)\n",
    "                obs_next, _, done, _ = self.env_val.step(action.item())\n",
    "                obs = obs_next\n",
    "                if done:\n",
    "                    price = self.env_val.prices.price[self.env_val.offset]\n",
    "                    hold = self.env_val.hold\n",
    "                    if hold:\n",
    "                        self.env_val.fund += price * hold\n",
    "                        if int(price * hold * 0.001425) == 0:\n",
    "                            tax = 1\n",
    "                        else:\n",
    "                            tax = int(price * hold * 0.001425)\n",
    "                        self.env_val.fund -= (tax + int(price*hold*0.003))\n",
    "                    funds += self.env_val.fund\n",
    "                    break\n",
    "                    \n",
    "        mean_profit = ((funds/len(data_val)) - 100000) / 100000\n",
    "        return mean_profit    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb7a35be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=241, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Stocks = Enviroment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa47dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuanf\\AppData\\Local\\Temp\\ipykernel_38004\\1336336076.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  state = torch.tensor([obs]).to(device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mStocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mEnviroment.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     25\u001b[0m state_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([obs_next])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mmemorize(state, action, state_next, reward)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_q_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m state_next\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mAgent.update_q_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_q_function\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mBrain.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_minibatch()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_state_action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_q_values()\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_main_q_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mBrain.update_main_q_network\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     67\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:307\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Stocks.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61540aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
